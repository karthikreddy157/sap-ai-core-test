apiVersion: ai.sap.com/v1alpha1
kind: ServingTemplate
metadata:
  name: image-inference-serving
  annotations:
    scenarios.ai.sap.com/name: "ImageInference"
    scenarios.ai.sap.com/description: "Image Inference API"
    executables.ai.sap.com/name: "image-infer"
    executables.ai.sap.com/description: "FastAPI Inference Service"
  labels:
    ai.sap.com/version: "1.0"
spec:
  inputs:
    parameters:
      - name: MODEL_IMAGE
        description: Docker image for inference service
        default: "karthikatsaartha/image-inference:1.0"
      - name: CONTAINER_PORT
        description: Service port
        default: "8080"
    artifacts:
      - name: modelArtifact
        description: ONNX model artifact
        path: /mnt/models/model.onnx

  template:
    apiVersion: serving.kserve.io/v1beta1
    kind: InferenceService
    metadata:
      labels:
        ai.sap.com/resourcePlan: starter
    spec:
      predictor:
        containers:
          - name: image-inference
            image: "{{ .Values.MODEL_IMAGE }}"
            env:
              - name: MODEL_PATH
                value: "/mnt/models/model.onnx"
            ports:
              - name: http
                containerPort: {{ .Values.CONTAINER_PORT }}
